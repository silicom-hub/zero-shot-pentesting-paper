import argparse
import gym
import nasim
import os
import numpy as np

from sb3_contrib import RecurrentPPO
from stable_baselines3 import DQN, A2C
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common import results_plotter
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results
from stable_baselines3.common.noise import NormalActionNoise
from stable_baselines3.common.callbacks import BaseCallback

import sys
sys.path.append("./")

from envs.nasim import Env_Nasim
from benchmark import Benchmark


class SaveOnBestTrainingRewardCallback(BaseCallback):
	"""
	Callback for saving a model (the check is done every ``check_freq`` steps)
	based on the training reward (in practice, we recommend using ``EvalCallback``).

	:param check_freq: (int)
	:param log_dir: (str) Path to the folder where the model will be saved.
		It must contains the file created by the ``Monitor`` wrapper.
	:param verbose: (int)
	"""
	def __init__(self, check_freq: int, log_dir: str, verbose=1):
		super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)
		self.check_freq = check_freq
		self.log_dir = log_dir
		self.save_path = os.path.join(log_dir, 'best_model')
		self.best_mean_reward = -np.inf

	def _init_callback(self) -> None:
		pass

	def _on_step(self) -> bool:
		if self.n_calls % self.check_freq == 0:

			# Retrieve training reward
			x, y = ts2xy(load_results(self.log_dir), 'timesteps')
			if len(x) > 0:
				# Mean training reward over the last 100 episodes
				mean_reward = np.mean(y[-100:])
				if self.verbose > 0:
					print("Num timesteps: {}".format(self.num_timesteps))
					print("Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}".format(self.best_mean_reward, mean_reward))

				# New best model, you could save the agent here
				if mean_reward > self.best_mean_reward:
					self.best_mean_reward = mean_reward
					# Example for saving best model
					if self.verbose > 0:
						print("Saving new best model to {}".format(self.save_path))
					self.model.save(self.save_path)
		return True



if __name__ == "__main__":
	parser = argparse.ArgumentParser()
	parser.add_argument("--env", type=str, default="tiny", help="the name of the environment")
	parser.add_argument("--agent", type=str, default="ppo_recurrent", help="the name of the agent to test (ppo_recurrent, dqn or a2c)")
	parser.add_argument("--timesteps", type=int, default=50000, help="the number of steps to do by the agent")

	args = parser.parse_args()

	env = Env_Nasim(args.env)
	env = gym.wrappers.RecordEpisodeStatistics(env)
	
	log_dir = "models2/%s/%s/" % (args.env, args.agent)
	os.makedirs(log_dir, exist_ok=True)

	env = Monitor(env, log_dir)
	callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)

	benchmark = Benchmark("output/results_test_v6.json")
	
	if args.agent == "ppo_recurrent":
		model = RecurrentPPO("MlpLstmPolicy", env, device="cuda", verbose=1)
	elif args.agent == "dqn":
		model = DQN("MlpPolicy", env, device="cuda", verbose=1)
	elif args.agent == "a2c":
		model = A2C("MlpPolicy", env, device="cuda", verbose=1)
	else:
		print("Please select a valid agent: 'dqn' or 'ppo_recurrent' !")
		raise NotImplementedError

	model.learn(args.timesteps, callback=callback)

	del model
	del env

	# env = Env_Nasim(args.env, mode="eval")
	env = Env_Nasim(args.env)

	if args.agent == "ppo_recurrent":
		model = RecurrentPPO.load(log_dir+"best_model", env=env, custom_objects = {'observation_space': env.observation_space, 'action_space': env.action_space})
	elif args.agent == "dqn":
		model = DQN.load(log_dir+"best_model", env=env, custom_objects = {'observation_space': env.observation_space, 'action_space': env.action_space})
	elif args.agent == "a2c":
		model = A2C.load(log_dir+"best_model", env=env, custom_objects = {'observation_space': env.observation_space, 'action_space': env.action_space})
	else:
		print("Please select a valid agent: 'dqn' or 'ppo_recurrent' !")
		raise NotImplementedError

	obs = env.reset()

	lstm_states = None
	num_envs = 1
	episode_starts = np.ones((num_envs, ), dtype=bool)
	episode_return = 0
	done = False

	i = 0
	while done == False:
		if args.agent == "ppo_recurrent":
			action, lstm_states = model.predict(obs, episode_start=episode_starts, state=lstm_states)
			obs, reward, done, info = env.step(action)
			episode_starts = done

		elif args.agent == "dqn":
			action, _ = model.predict(obs)
			obs, reward, done, info = env.step(action)

		elif args.agent == "a2c":
			action, _states = model.predict(obs)
			obs, reward, done, info = env.step(action)

		else:
			raise NotImplementedError

		name_action = env.nasim.action_space.get_action(action).name
		target_action = env.nasim.action_space.get_action(action).target

		print("Action nÂ°%d: %s on %s" % (i, name_action, target_action))
		i += 1
		episode_return += reward

	benchmark.add_new_experiment(args.env, args.agent, episode_return)
	
	benchmark.update_scores()
	benchmark.render()
	benchmark.save()

	print("Episode return = %d" % episode_return)